---
title: "Peekbank Item Prediction"
author: "Martin Z"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    toc: true
    toc_depth: 4
    toc_float: true
---


```{r, message=F, warning=F}
library(knitr)
library(peekbankr)
library(tidyverse)
library(lme4)
library(lmerTest)
library(tictoc)
library(langcog)
library(here)
#library(tidymodels)
library(wordbankr)
#devtools::install_github("tidymodels/multilevelmod")
#library(multilevelmod)

figure_path <- here("figures")

load(file = "data/aoi_data_joined.Rds")
knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE)
dataset_name_mapping <- read_csv(here("data","dataset_name_mapping.csv"))

con <- connect_to_peekbank()
stimuli <- get_stimuli(connection = con) %>% collect()
subjects <- get_subjects(connection = con) %>% collect()

aoi_data_joined <- aoi_data_joined %>%
  left_join(subjects)


```

The goal is to explore relationships between item-level properties (specifically, child-directed word frequency and age of acquisition) and accurate looking in the looking while listening paradigm across age.

## Preparing the data 

### Fit and filter trial accuracy (proportion looking to target)

Compute trial-level accuracy (proportion looking to the target) in a fixed critical window (300-2000ms).

We are focusing only on 

* familiar words

* ages between 12-60 months

* English datasets (since this is what we have norms for)

```{r}
t_min <- 300
t_max <- 2000

by_trial_means <- aoi_data_joined %>%
  #remove dataset
  #filter(dataset_name!="pomper_saffran_2016") %>%
  #restrict to english datasets
  filter(native_language == "eng") %>%
  #restrict age range
  filter(age > 12, age <= 60) %>%
  # familiar target items only %>%
  filter(stimulus_novelty == "familiar") %>%
  #window of analysis
  filter(t_norm > t_min, t_norm < t_max) %>%
  mutate(age_binned = cut(age, seq(12,60,12))) %>%
  rename(target_label = english_stimulus_label) %>%
  group_by(administration_id, trial_id, target_label, distractor_id, 
           age, age_binned) %>%
  summarise(prop_looking = sum(aoi == "target", na.rm = TRUE) / 
              (sum(aoi == "target", na.rm=TRUE) + 
                 sum(aoi=="distractor", na.rm=TRUE)),
            prop_missing = mean(aoi == "missing", na.rm = TRUE)) %>%
  left_join(stimuli, by = c("distractor_id" = "stimulus_id")) %>%
  rename(distractor_label = english_stimulus_label)
  
```

Filter the data to trials where there is sufficient looking data (target or distractor looking on at least 2/3 of the trial)

```{r}
acc_mod_data <- by_trial_means %>%
  ungroup() %>%
  filter(prop_missing < 1/3) %>%
  mutate(age_centered = age - mean(age,na.rm=TRUE))

target_label_counts <- acc_mod_data %>%
  ungroup() %>%
  group_by(target_label) %>%
  summarize(
    trial_count=n()
  )

acc_mod_data <- acc_mod_data %>%
  left_join(target_label_counts)

```

### Add English Frequency Info

TO DO: re-extract frequency information using childes-db. Source for the current frequency values: https://github.com/mikabr/aoa-prediction

```{r}
freq <- read_csv(here("data/childes_english.csv"))

acc_mod_freq <- left_join(acc_mod_data, 
                      select(freq,word, word_count),by = c("target_label" = "word")) %>%
  rename(target_word_count=word_count) %>%
  left_join(select(freq,word, word_count),by = c("distractor_label" = "word")) %>%
  rename(distractor_word_count=word_count) %>%
  ungroup() %>%
  mutate(target_log_freq = log(target_word_count), 
         distractor_log_freq = log(distractor_word_count)) %>%
  mutate(target_log_freq_centered = target_log_freq-mean(target_log_freq,na.rm=TRUE),
         distractor_log_freq_centered = distractor_log_freq-mean(distractor_log_freq,na.rm=TRUE)) %>%
  mutate(luce_log_freq = target_log_freq/ (target_log_freq+distractor_log_freq))

by_target_item_means <- acc_mod_freq %>%
  ungroup() %>%
  group_by(target_label,target_word_count,target_log_freq,age_binned) %>%
  summarise(
    N=n(),
    mean_prop_looking=mean(prop_looking,na.rm=TRUE)
  )


  
```

### Collect English AOA Information

Run this code chunk to collect AOA information from the English (American) WS form on Wordbank. After identifying target labels that cannot be found on Wordbank, a .csv is exported to resolve inconsistencies by hand (e.g. "chicken" == "chicken (animal)" in Wordbank).

```{r,eval = FALSE}
items_for_aoa <- unique(acc_mod_data$target_label)
#get wordbank items
wordbank_items_eng_ws <- get_item_data(language = "English (American)", form = "WS")
#compare to target label names
setdiff(items_for_aoa,wordbank_items_eng_ws$definition)
#output set difference for manual processing
write_csv(data.frame(target_label=setdiff(items_for_aoa,wordbank_items_eng_ws$definition)),here("data","items_dropped_aoa.csv"))

#set up and read in mapping file w/ aligned definitions for worddbank
stimulus_label_wordbank_intersect <- data.frame(
  target_label=intersect(items_for_aoa,wordbank_items_eng_ws$definition),
  definition=intersect(items_for_aoa,wordbank_items_eng_ws$definition))
stimulus_label_wordbank_mapping <- read_csv(here("data","stimulus_label_wordbank_mapping.csv"))
stimulus_label_wordbank <- bind_rows(stimulus_label_wordbank_intersect,stimulus_label_wordbank_mapping)

item_names_for_wordbank <- stimulus_label_wordbank$definition[!is.na(stimulus_label_wordbank$definition)]

items_for_wordbank <- filter(wordbank_items_eng_ws,definition %in% c(item_names_for_wordbank))$item_id

#get instrument data for target label items from wordbank
eng_ws_data <- get_instrument_data(language = "English (American)",
                                   form = "WS",
                                   items = items_for_wordbank,
                                   administrations=TRUE,
                                   iteminfo=TRUE)

#fit AOA curves to obtain AOA estimates (logistic regression)
aoas_ws_produces <- fit_aoa(eng_ws_data,measure="produces", age_min=0) %>%
  ungroup() %>%
  select(aoa,item_id,definition) %>%
  left_join(stimulus_label_wordbank) %>%
  select(target_label,definition,aoa)

write_csv(aoas_ws_produces,here("data","aoas_wordbank_ws_produces.csv"))
```

### Add English AOAs

Join English AOAs into the trial-level accuracy data, both for target labels and for distractor labels.

```{r}
#aoas <- read_csv(here("data","bglm_aoas_english.csv")) 
aoas <- read_csv(here("data","aoas_wordbank_ws_produces.csv"))

acc_mod_freq_aoa <- left_join(acc_mod_freq, 
                      aoas) %>% #%>%
                        # transmute(target_label = definition, 
                        #           target_aoa = bglm_aoa, 
                        #           target_category = category)) %>%
  rename(target_aoa=aoa) %>%             
  left_join(aoas  %>%
              transmute(distractor_label = definition, 
                       distractor_aoa = aoa)) %>%
  filter(!is.na(target_aoa), !is.na(distractor_aoa)) %>%
  ungroup() %>%
  mutate(target_aoa_centered = target_aoa - mean(target_aoa,na.rm=TRUE), 
         distractor_aoa_centered = distractor_aoa - mean(distractor_aoa,na.rm=TRUE)) %>%
  mutate(inverse_target = 1/target_aoa,
         inverse_distractor = 1/distractor_aoa,
         luce_untransformed=target_aoa/(target_aoa+distractor_aoa),
         luce = inverse_target / (inverse_target + inverse_distractor),
         luce_log = log(inverse_target) / (log(inverse_target) + log(inverse_distractor)))
```

## Does item frequency predict target looking? {.tabset}

First, we're going to explore whether the log word frequency of the relevant items (from CHILDES) predicts proportion target looking. 
We'll look at just the target word frequency first, and then more complex models that also include distractor label frequency. 
The crux is that although it appears that there is some signal here (see plots below and model without random item effects), that signal doesn't generalize across items (i.e., once random effects for items are added into the model).
This is probably the result of a few different things coming together:

1. We have vastly different numbers of observations for different items

2. There are idiosyncrasies to these datasets, and one or two datasets with a distinct set of items can have a vast amount of influence at this point.

3. The items are not randomly distributed across e.g., age, but are specifically selected for the age range tested, so there is significant range restriction likely going on.

### Plots {.tabset}

#### general correlation

```{r}
#by age bin
acc_mod_freq %>%
  ggplot(aes(target_log_freq,prop_looking))+
  geom_point(alpha=0.01)+
  geom_smooth(method="lm")
```

#### By Age Bin

```{r}
#by age bin
acc_mod_freq %>%
  ggplot(aes(target_log_freq,prop_looking))+
  geom_point(alpha=0.01)+
  geom_smooth(method="lm")+
  facet_wrap(~age_binned)
```
#### By Dataset

```{r}
#by dataset
acc_mod_freq %>%
  ggplot(aes(target_log_freq,prop_looking))+
  geom_point(alpha=0.01)+
  geom_smooth(method="lm")+
  facet_wrap(~dataset_name)
```

#### Plot summarized Items

This plot shows relationships between frequency and proportion target looking when averaging across trials for each specific item.

```{r}
n_cutoff <- 20
by_target_item_means %>%
  filter(N>n_cutoff) %>%
  ggplot(aes(target_log_freq,mean_prop_looking,size=N,color=target_label))+
  geom_point()+
  geom_smooth(method="lm",color="black")+
  theme(legend.position="none")+
  facet_wrap(~age_binned)
```

### Target Frequency {.tabset}

Predict proportion looking from target frequency, controlling for age. The frequency effect does not appear generalize across items (i.e., it disappears once by-item random effects are included).

#### No item random effects

```{r}
m1 <- lmer(prop_looking ~ target_log_freq_centered+age_centered+(1|administration_id)+(1|dataset_name), data = acc_mod_freq)
summary(m1)
```

#### With item random effects

```{r}
m1A <- lmer(prop_looking ~ target_log_freq_centered+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label), data = acc_mod_freq)
summary(m1A)
```

#### Trial Count Threshold

I also considered restricting the data to just items where there is some minimum number of observations (in this case, 20), since I thought items with only small numbers of observations might be introducing a lot of noise. This does not appear to make a big difference to the models, however.

```{r}
m1A_red <- lmer(prop_looking ~ target_log_freq_centered+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label), data = filter(acc_mod_freq,trial_count>20))
summary(m1A_red)
```

### Target & Distractor Frequency {.tabset}

Fit more complex models including both target word and distractor word frequency. The issues remain similar.

#### No item random effects
```{r}
m2 <- lmer(prop_looking ~ target_log_freq_centered+distractor_log_freq_centered+age_centered+(1|administration_id)+(1|dataset_name), data = acc_mod_freq)
summary(m2)
```

#### Item random effects
```{r}
m2A <- lmer(prop_looking ~ target_log_freq_centered+distractor_log_freq_centered+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label)+(1|distractor_label), data = acc_mod_freq)
summary(m2A)
```

#### Trial Count Threshold

Restrict to items with >20 observations

```{r}
m2A_red <- lmer(prop_looking ~ target_log_freq_centered+distractor_log_freq_centered+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label)+(1|distractor_label), data = filter(acc_mod_freq,trial_count>20))
summary(m2A_red)
```

## Does AOA predict target looking? {.tabset}

Next, I focused on predicting proportion target looking on each trial from target AOA, and then subsequently more complex models also including distractor AOA. The issue of generalizing across items is very similar to the case of frequency, likely for similar reasons.

### Target AOA {.tabset}

Predict proportion target looking from target aoa controlling for age.

#### No item random effects

```{r}
m3 <- lmer(prop_looking ~ target_aoa_centered+age_centered+(1|administration_id)+(1|dataset_name), data = acc_mod_freq_aoa)
summary(m3)
```

#### With item random effects

```{r}
m3A <- lmer(prop_looking ~ target_aoa_centered+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label), data = acc_mod_freq_aoa)
summary(m3A)
```

#### Trial Count Threshold

```{r}
m3A_red <- lmer(prop_looking ~ target_aoa_centered+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label), data = filter(acc_mod_freq_aoa,trial_count>20))
summary(m3A_red)
```

#### Alternate Analysis: AOA - Age

We considered an alternate analysis in which we considered *relative aoa* (age - age of acquisition). While this is a strong predictor, this is likely largely due to the strong relationship between age and proportion target looking.


```{r}
#compute "relative" AOA
acc_mod_freq_aoa <- acc_mod_freq_aoa %>%
  mutate(relative_target_aoa=age-target_aoa,
         relative_distractor_aoa=age-distractor_aoa)

ggplot(acc_mod_freq_aoa,aes(relative_target_aoa,prop_looking))+
  geom_point()+
  geom_smooth()
```

```{r}
##target alone
m_diff_target <- lmer(prop_looking ~ relative_target_aoa+(1|administration_id)+(1|dataset_name)+(1|target_label)+(1|distractor_label), data = acc_mod_freq_aoa)
summary(m_diff_target)

##distractor alone
m_diff_distractor <- lmer(prop_looking ~ relative_distractor_aoa+(1|administration_id)+(1|dataset_name)+(1|target_label)+(1|distractor_label), data = acc_mod_freq_aoa)
summary(m_diff_distractor)

## target and distractor
m_diff_td <- lmer(prop_looking ~ relative_target_aoa+relative_distractor_aoa+(1|administration_id)+(1|dataset_name)+(1|target_label)+(1|distractor_label), data = acc_mod_freq_aoa)
summary(m_diff_td)

```

### Target & Distractor AOA {.tabset}

#### No item random effects
```{r}
m4 <- lmer(prop_looking ~ target_aoa_centered+distractor_aoa_centered+age_centered+(1|administration_id)+(1|dataset_name), data = acc_mod_freq_aoa)
summary(m4)
```

#### Item random effects
```{r}
m4A <- lmer(prop_looking ~ target_aoa_centered+distractor_aoa_centered+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label)+(1|distractor_label), data = acc_mod_freq_aoa)
summary(m4A)
```

#### Trial Count Threshold

```{r}
m4A_red <- lmer(prop_looking ~ target_aoa_centered+distractor_aoa_centered+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label)+(1|distractor_label), data = filter(acc_mod_freq_aoa,trial_count>20))
summary(m4A_red)
```

### Luce Choice AOA {.tabset}

Using Mike Frank's suggestion for a Luce choice predictor (1/target_aoa)/(1/target_aoa+1/distractor_aoa)

#### No item random effects

```{r}
m4B <- lmer(prop_looking ~ luce+age_centered+(1|administration_id)+(1|dataset_name), data = acc_mod_freq_aoa)
summary(m4B)
```
#### With item random effects

```{r}
m4C <- lmer(prop_looking ~ luce+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label)+(1|distractor_label), data = acc_mod_freq_aoa)
summary(m4C)
```


## Combined Effect of Frequency and AOA {.tabset}

Finally, we combined all frequency and AOA predictors into a single model. Again, almost all putative effects do not "survive" including by-item random effects (with the exception of perhaps target word AOA). 

### no item random effects
```{r}
m5 <- lmer(prop_looking ~ target_log_freq_centered+distractor_log_freq_centered+target_aoa_centered+distractor_aoa_centered+age_centered+(1|administration_id)+(1|dataset_name), data = acc_mod_freq_aoa)
summary(m5)
```


### item random effects

Target AOA is the only effect that remains significant after accounting for non-independence between items (targets and distractors).

```{r}
m5A <- lmer(prop_looking ~ target_log_freq_centered+distractor_log_freq_centered+target_aoa_centered+distractor_aoa_centered+age_centered+(1|administration_id)+(1|dataset_name)+(1|target_label)+(1|distractor_label), data = acc_mod_freq_aoa)
summary(m5A)
#vif.mer(m5A)
```




