---
title: "Trial analysis 4: predicting item variation"
author: "Mike"
date: "2/19/2022"
output: html_document
---

```{r setup, echo = FALSE}
suppressPackageStartupMessages(library(here))
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(peekbankr))
suppressPackageStartupMessages(library(lme4))
suppressPackageStartupMessages(library(ggpmisc))
suppressPackageStartupMessages(library(ggrepel))
suppressPackageStartupMessages(library(ggthemes))

# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed, cache = TRUE, 
                      message=FALSE, warning=FALSE, error=FALSE)

load(file = here("explorations","data","d_trial.Rds"))
load(file = here("explorations","data","poly_mod.Rds"))

df <- d_trial |>
    filter(t_norm > 500, t_norm < 4000) |>
    group_by(dataset_name, dataset_id, administration_id, 
             age, stimulus_id, target_label, distractor_label) |>
    summarise(accuracy = mean(correct[t_norm > 0], na.rm=TRUE),
              prop_data = mean(!is.na(correct[t_norm > 0]))) 
df <- df[complete.cases(df),]
```

# Goal 2: Compare to Wordbank AoAs

Load AoAs - this is from `aoa_prediction.R`, which I assume I wrote at some point but can't remember. 

```{r get_aoas, eval=FALSE}
items <- wordbankr::get_item_data(language = "English (American)") 

ws_data <- wordbankr::get_instrument_data(language = "English (American)", 
                                          form = "WS", 
                                          administrations = TRUE, 
                                          items = items$item_id[items$form == "WS"]) %>%
  right_join(items)
wg_data <- wordbankr::get_instrument_data(language = "English (American)", 
                                          form = "WG", 
                                          administrations = TRUE, 
                                          items = items$item_id[items$form == "WG"]) %>%
  right_join(items)

wordbank_data <- bind_rows(ws_data, wg_data) %>%
  mutate(produces = value == "produces", 
         form = "both", 
         num_item_id = definition, # stupid stuff to make fit_aoa work on joint data
         item_id = definition) %>%
  filter(type == "word")

aoas <- wordbankr::fit_aoa(wordbank_data, 
                           measure = "produces", 
                           method = "glmrob",
                           age_min = 8, 
                           age_max = 36)
saveRDS(aoas, here("explorations","data","aoas.rds"))
```

Load these from cache since it's time-consuming. 

```{r aoas}
aoas <- readRDS(here("explorations","data","aoas.rds"))
```

Now let's look at the relationship between AoAs and accuracies. 

## Naive AoA-accuracy

What's the right metric to compare? Here are some ideas:

1. average accuracy (confounded with age and study)
2. accuracy random intercepts
3. predicted point at which accuracy is greater than XYZ%

Let's try each. First let's do average accuracy. 

```{r}
mdf <- df |>
  group_by(target_label) |>
  summarise(avg_accuracy = mean(accuracy),
            n_trials = n()) |>
  inner_join(aoas |>
              ungroup() |>
              select(definition, aoa) |>
              rename(target_label = definition))

ggplot(mdf, aes(x = aoa, y = avg_accuracy)) + 
  geom_point(alpha = .5) + 
  geom_smooth(method = "lm") + 
  ggpmisc::stat_correlation() + 
  ggrepel::geom_label_repel(aes(label = target_label))
```
There's a correlation, but it goes in the WRONG direction! That's probably because studies for older kids have harder words. 

Let's try methods 2 and 3.

```{r}
poly_labels <- tibble(ranef(poly_mod)$target_label) |>
  rename(poly_intercept = `(Intercept)`,
         poly_1 = `poly(age_scaled, 2)1`,
         poly_2 = `poly(age_scaled, 2)2`)
poly_labels$target_label <- rownames(ranef(poly_mod)$target_label) 

mdf <- left_join(mdf, poly_labels)
```

Extract "AoAs" from predicted curves. 

```{r}
mod_aoas <- expand_grid(age_scaled = seq(min(df$age_scaled), max(df$age_scaled), .01), 
                       target_label = mdf$target_label)
mod_aoas$pred <- predict(poly_mod, 
                        newdata = mod_aoas, 
                        re.form = ~(poly(age_scaled,2) | target_label), 
                        type = "response")
mod_aoas$age <- (mod_aoas$age_scaled * sd(df$age)) + mean(df$age)

mdf <- left_join(mdf, 
                 mod_aoas |>
                   group_by(target_label) |>
                   summarise(aoa_60 = age[pred > .60][1],
                             aoa_65 = age[pred > .65][1],
                             aoa_70 = age[pred > .70][1],
                             aoa_75 = age[pred > .75][1]))
```

Now plot all of these. 

```{r}         
mdf_long <- mdf |> 
  pivot_longer(cols = -c("target_label", "aoa", "n_trials"), 
               names_to = "predictor", 
               values_to = "value")
ggplot(mdf_long, 
       aes(x = aoa, y = value)) + 
  geom_point(alpha = .5) + 
  geom_smooth(method = "lm") + 
  ggpmisc::stat_correlation() + 
  ggrepel::geom_label_repel(aes(label = target_label)) + 
  facet_wrap(~predictor, scales="free_y")
```

Ugh. Surprising that these are not more related. Let's look in on the one that seems to have ANY downward signal. Let's zoom in. 

```{r}
ggplot(filter(mdf_long, predictor == "poly_2"),
       aes(x = aoa, y = value)) + 
  geom_point(alpha = .5, aes(size = n_trials)) + 
  geom_smooth() + 
  ggpmisc::stat_correlation() + 
  ggrepel::geom_label_repel(aes(label = target_label)) 
```
We see that we have very little data about most of the words here, so there is a huge missing data issue. 

What is Poly2? 

```{r}
qplot(df$age_scaled, poly(df$age_scaled, 2)[,2])
```


<!-- Let's zoom in on the ones we have most data on. Surprisingly, this shows that both the 70% point of the AOA curves and the HL exponent do correlate negatively (expected direction).  -->

<!-- ```{r} -->
<!-- ggplot(filter(mdf_long, predictor %in% c("aoa_60", "poly_2"), -->
<!--               n_trials > 100), -->
<!--        aes(x = aoa, y = value)) +  -->
<!--   geom_point(alpha = .5, aes(size = n_trials)) +  -->
<!--   geom_smooth() +  -->
<!--   ggpmisc::stat_correlation() +  -->
<!--   ggrepel::geom_label_repel(aes(label = target_label)) +  -->
<!--   facet_wrap(~predictor, scales = "free_y") -->
<!-- ``` -->
<!-- Tentative conclusion is that we have a problem, which is that lots of harder words are shown to small samples of older children (who are better at recognizing them). This means the random effects are not very useful here.  -->

<!-- But I'd still like to zoom in more on why the estimates are so low here for book, shoe, juice.  -->


<!-- # New directions -->

<!-- So that all looked kind of terrible. Maybe we need better models of the confounding: -->

<!-- * limited numbers of items shared across experiments -->
<!-- * harder words for older kids -->
<!-- * different distractors -->

<!-- ## dataset random effects -->


<!-- ```{r} -->
<!-- hl_mod <- lmer(accuracy ~ I(.5 + .5 * (1 / (1 + exp(1)^(age_scaled)))) + (1 | administration_id) +  -->
<!--        (I(.5 + .5 * (1 / (1 + exp(1)^(age_scaled))))| target_label) +  -->
<!--        (1 | dataset_name),  -->
<!--      data = df) -->

<!-- summary(hl_mod) -->
<!-- ``` -->


<!-- ## model of choice - what's the distractor?  -->

