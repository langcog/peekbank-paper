---
title: "Peekbank developmental curves"
author: "Mike"
date: "1/19/2021"
output: html_document
---

```{r}
library(peekbankr)
library(lme4)
library(tictoc)
library(langcog)
library(here)
library(tictoc)
library(broom)
library(tidyverse)
library(cowplot)
library(JuliaCall)
library(glue)
library(broom)
source(here("misc/jglmm_helper.R"))


theme_set(theme_bw() + 
            theme(strip.background = element_blank(), 
                  panel.grid = element_blank()))

knitr::opts_chunk$set(cache = FALSE, warn = FALSE, message = FALSE)
```


Goal is to make developmental curves by using the growth modeling approach advocated by Mirman (2014).  

```{r}
load(file = "data/aoi_data_joined.Rds")
```

# Data

First take only familiar word data. 

```{r}
fam_data <- aoi_data_joined %>%
  filter(dataset_name != "casillas_tseltal_2015") %>%
  filter(age > 12, age <= 60, 
         stimulus_novelty == "familiar") %>%
  mutate(age_binned = cut(age, seq(0,60,12))) 
```

Check that the general curves look good. 

```{r}
means <- fam_data %>%
  group_by(t_norm, dataset_name, age_binned, stimulus_novelty) %>%
  summarise(n = sum(aoi %in% c("target","distractor"), na.rm = TRUE), 
            p = sum(aoi == "target", na.rm = TRUE),
            prop_looking = p / n, 
            ci_lower = binom::binom.confint(p, n, method = "bayes")$lower,
            ci_upper = binom::binom.confint(p, n, method = "bayes")$upper) 

ggplot(means, 
       aes(x = t_norm, y = prop_looking)) + 
  geom_line(aes(col = dataset_name)) + 
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper, 
                  fill = dataset_name), alpha = .5) +
  facet_grid(age_binned~stimulus_novelty) +
  geom_hline(yintercept = .5, lty = 2) + 
  geom_vline(xintercept = 0, lty = 2) +
  ylab("Proportion Target Looking") +
  xlab("Time (msec)") +
  theme_classic() +
  scale_color_solarized() +
  scale_fill_solarized() 
```

While the curves look good, the words are very different across datasets and so the curves don't line up perfectly. You need models to deal with this situation. 

# Orthogonal polynomials, first pass

We're going to use the empirical logit modeling approach from Mirman (2014), where we analyze 

$$elogit(y,N) = \log(\frac{y + .5}{N - y + .5})$$
as a linear variable binned by time. In practice the histogram doesn't look remotely linear but it allows us to use linear mixed models rather than logistic models and the logistics right now don't fit with this much data... 

```{r}
BIN_INTERVAL <- 100
T_RANGE <- c(0,1500)

ms_timecourse <- fam_data %>%
  filter(aoi %in% c("target","distractor"), 
         # dataset_name %in% c("perry_cowpig","mahr_coartic"), 
         t_norm >= T_RANGE[1],
         t_norm <= T_RANGE[2]) %>%
  mutate(t_window = 
           as.numeric(as.character(
             cut(t_norm, 
                          breaks = seq(T_RANGE[1],T_RANGE[2],BIN_INTERVAL), 
                          labels = seq(T_RANGE[1] + BIN_INTERVAL / 2, 
                                       T_RANGE[2] - BIN_INTERVAL / 2, 
                                       BIN_INTERVAL)))),
         age_centered = age - 36) %>%
  group_by(dataset_id, administration_id, trial_id, t_window, 
           age, age_centered, age_binned, english_stimulus_label) %>%
  summarise(prop_target = round(mean(aoi=="target")), 
            num_target = sum(aoi == "target"), 
            N = length(aoi), 
            elogit = log( (num_target + .5) / (N - num_target + .5)), 
            wts = 1/(num_target + .5) + 1/(N - num_target + .5)) %>%
  filter(!is.na(t_window))

hist(ms_timecourse$prop_target)
hist(ms_timecourse$elogit)
```

Let's visualize the data going into our model, just for kicks. We can see why this is tricky. 

```{r}
ggplot(ms_timecourse, 
       aes(x = t_window, y = elogit, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(geom = "line") +
  xlab("Time (ms)") +
  ylab("Target Looking (elogit)") 
```

Now we make our orthogonal polynomials with code straight from Mirman (2014). Make more than we need so we can try out different degrees. 

```{r}
POLY_DEG <- 5

ops <- poly(unique(ms_timecourse$t_window), POLY_DEG)
ops_tibble <- tibble(ot1 = ops[,1], 
                     ot2 = ops[,2],
                     ot3 = ops[,3],
                     ot4 = ops[,4],
                     ot5 = ops[,5],
                     t_window = unique(ms_timecourse$t_window))

ms_timecourse <- left_join(ms_timecourse, ops_tibble)
```
Plot these to understand them better. 

```{r}
ops_tibble %>%
  pivot_longer(starts_with("ot")) %>%
  ggplot(aes(x = t_window, y = value)) + 
  geom_line() +
  facet_wrap(~name)
```


# Models

We explore a glmer over the discretized proportions. This fits slowly and has some convergence issues even when we prune to the most minimal random effect structure. I wasn't able to get any random slopes in there in particular, which seems like a deal-breaker. So let's skip this. 

```{r, eval = FALSE}

mod_prop <- glmer(prop_target ~ (ot1 + ot2 + ot3) * age_centered +
                     (1 | dataset_id) +
                     (1 | english_stimulus_label),
                  family = "binomial", 
                  data = ms_timecourse)

```

The elogit is quite similar in distribution... not sure we can legitimately use a linear link, but we adopt it for now.

```{r}
tic()
elogit <- lmer(elogit ~ (ot1 + ot2 + ot3) * age_centered +
                  (1 | administration_id) +
                  (1 | dataset_id) +
                  (1 | english_stimulus_label),
                weights = 1/wts, 
                data = ms_timecourse)
toc()
```

Let's look at the model. 

```{r}
summary(elogit)
```

Now, let's look at model fits. 

```{r}
elogit_data = ms_timecourse
elogit_data$fit <- fitted(elogit)

ggplot(elogit_data, 
       aes(x = t_window, y = elogit, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(aes(y = fit), fun = mean, geom = "line") +
  geom_hline(yintercept = 0, col = "black", lty = 2) +
  # ylim(0, 1) +
  xlab("Time (ms)") +
  ylab("Target Looking (elogit)") 
```
So x is the elogit value. 

$$x = \log(\frac{y + .5}{N - y + .5})$$

So solve for y

$$\frac{e^x(2N + 1) - 1}{2(e^x + 1)} = y  $$

The `VWpre` package has a function, but it assumes constant N. 

```{r}
elogit_to_prop <- function(x, N) {
  y = ((exp(x) * (2 * N + 1) - 1) / (2 * (exp(x) + 1))) / N
  
  return(y)
}
```

Plot in proportion space. 

```{r}
# try VWPRE
elogit_data$fitted_prop <- elogit_to_prop(elogit_data$fit, elogit_data$N)
ggplot(elogit_data, 
       aes(x = t_window, y = prop_target, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(aes(y = fitted_prop), fun = mean, geom = "line") +
  geom_hline(yintercept = .5, col = "black", lty = 2) +
  # ylim(0, 1) +
  xlab("Time (ms)") +
  ylab("Proportion Target Looking") 
```


Plot model predictions from fixed effects. **TODO: quadratic age effects**

```{r}
newdata <- left_join(expand_grid(t_window = seq(50, 1450, 100), 
                            age_centered = c(18,30, 42, 54) - 36), 
                            ops_tibble) 

newdata$pred <- predict(elogit, newdata = newdata, re.form = NA)
newdata$fitted_prop <- elogit_to_prop(newdata$pred, 4)

ggplot(newdata, 
       aes(x = t_window, y = pred, col = factor(age_centered+36))) + 
  geom_line()
```


```{r}
tic()
elogit_explore <- lmer(elogit ~ 0 + t_window + 
                         I(t_window^2) + I(t_window^3) + 
                         t_window:age_centered + 
                         I(t_window^2):age_centered + 
                         I(t_window^3):age_centered + 
                       #(ot1 + ot2 + ot3) * age_centered +
                         # age_centered + 
                  (1 | administration_id) +
                  (1 | dataset_id) +
                  (1 | english_stimulus_label),
                weights = 1/wts, 
                data = ms_timecourse)
toc()

elogit_data$fit <- fitted(elogit_explore)

elogit_data$fitted_prop <- elogit_to_prop(elogit_data$fit, elogit_data$N)
p1 <- ggplot(elogit_data, 
       aes(x = t_window, y = elogit, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(aes(y = fit), fun = mean, geom = "line") +
  geom_hline(yintercept = 0, col = "black", lty = 2) +
  scale_color_solarized() + 
  # ylim(0, 1) +
  xlab("Time (ms)") +
  ylab("Proportion Target Looking") + 
  theme(legend.position = "bottom")

newdata$pred <- predict(elogit_explore, newdata = newdata, re.form = NA)
newdata$pred_prop <- elogit_to_prop(newdata$pred, 4)

p2 <- ggplot(newdata, 
       aes(x = t_window, y = pred, col = factor(age_centered+36))) + 
  geom_line() + 
  geom_hline(yintercept = 0, col = "black", lty = 2) +
  scale_color_solarized(name = "Age") + 
  xlab("Time (ms)") +
  ylab("Proportion Target Looking") + 
  theme(legend.position = "bottom")

cowplot::plot_grid(p1, p2)
```


# Julia version

Performance is slow here, and convergence is imperfect, so we can't really add too much more in the way of bells and whistles, but I think we need them! So let's try Julia.

## Getting Julia up and running

This step takes a while the first time you run it. 

```{r}
library(jglmm)
options(JULIA_HOME = "/Applications/Julia-1.5.app/Contents/Resources/julia/bin")
jglmm_setup()
```


```{r}
ms_timecourse$administration_id <- as.character(ms_timecourse$administration_id)
ms_timecourse$dataset_id <- as.character(ms_timecourse$dataset_id)
tic()
elogit_jl <- jglmm(elogit ~ (ot1 + ot2 + ot3) * age_centered +
                  (1 | administration_id) +
                  (1 | dataset_id) +
                  (1 | english_stimulus_label),
                family = "normal",
                weights = 1/ms_timecourse$wts, 
                data = ms_timecourse)
toc()
```

Surprisingly, dataset ID has very low variance. That suggests that maybe we don't want to model that as carefully? Or perhaps it's because a simple intercept of dataset doesn't really tell you much and some random effects would be good? 

```{r}
elogit_jl$model
```

The coefficients on this one match the one above almost exactly. 

```{r}
elogit_jl_data <- augment(elogit_jl)

ggplot(elogit_jl_data, 
       aes(x = t_window, y = elogit, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(aes(y = .fitted), fun = mean, geom = "line") +
  geom_hline(yintercept = 0, col = "black", lty = 2) +
  # ylim(0, 1) +
  xlab("Time (ms)") +
  ylab("Target Looking (elogit)") 
```

Now we have a workflow that lets us fit a bigger class of models faster!

## Exploration

```{r}
tic()
elogit_jl_4 <- jglmm(elogit ~ (ot1 + ot2 + ot3 + ot4) * age_centered +
                  (1 | administration_id) +
                  (1 | dataset_id) +
                  (1 | english_stimulus_label),
                family = "normal",
                weights = 1/ms_timecourse$wts, 
                data = ms_timecourse)
toc()

elogit_jl_4_data <- augment(elogit_jl_4)

ggplot(elogit_jl_4_data, 
       aes(x = t_window, y = elogit, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(aes(y = .fitted), fun = mean, geom = "line") +
  geom_hline(yintercept = 0, col = "black", lty = 2) +
  # ylim(0, 1) +
  xlab("Time (ms)") +
  ylab("Proportion Target Looking") 
```


```{r}
tic()
elogit_jl_5 <- jglmm(elogit ~ (ot1 + ot2 + ot3 + ot4 + ot5) * age_centered +
                  (1 | administration_id) +
                  (1 | dataset_id) +
                  (1 | english_stimulus_label),
                family = "normal",
                weights = 1/ms_timecourse$wts, 
                data = ms_timecourse)
toc()

elogit_jl_5_data <- augment(elogit_jl_5)

ggplot(elogit_jl_5_data, 
       aes(x = t_window, y = elogit, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(aes(y = .fitted), fun = mean, geom = "line") +
  geom_hline(yintercept = 0, col = "black", lty = 2) +
  # ylim(0, 1) +
  xlab("Time (ms)") +
  ylab("Proportion Target Looking") 
```

```{r}
elogit_jl$model
elogit_jl_4$model
elogit_jl_5$model
```

Conclusion, even 5th order polynomials don't completely capture all the weird stuff going on here? BIC actually increases though, so let's not retain these. Interestingly, we only see interactions of age and ot1 and ot3. 

Now let's try adding some more random effects. Dataset age x time slopes feel important here. Tried adding random effects under dataset but totally struck out. 

Following  https://github.com/JuliaStats/MixedModels.jl/issues/127 I tried adding dataset_id as a fixed effect instead and that works but is gross. We would have to do some kind of coding so that the other effects are at the average rather than dummy coding...

After discussion, seems like we can have random slopes but only if we want one factor. 

```{r}
tic()
elogit_jl_re_stim <- jglmm(elogit ~ (ot1 + ot2 + ot3) * age_centered  + 
                  # (1 | administration_id) +
                  # (1 | dataset_id) + 
                  (age_centered | english_stimulus_label),
                family = "normal",
                weights = 1/ms_timecourse$wts, 
                data = ms_timecourse)
toc()
```

```{r}
tic()
elogit_jl_re_dataset <- jglmm(elogit ~ (ot1 + ot2 + ot3) * age_centered  + 
                  # (1 | administration_id) +
                  (age_centered | dataset_id),
                  # (age_centered | english_stimulus_label),
                family = "normal",
                weights = 1/ms_timecourse$wts, 
                data = ms_timecourse)
toc()


```
```{r}
elogit_jl_re_stim$model
elogit_jl_re_dataset$model
```

Comparing BICs, we see that the item random effect is more important. That said, there feel like there are big dataset differences here, can we soak these up? 

```{r}
tic()
elogit_jl_re_dataset_more <- 
  jglmm(elogit ~ 
          (ot1 + ot2 + ot3 + ot4) * age_centered +     
          ((ot1 + ot2 + ot3 + ot4) * age_centered  | dataset_id),
                family = "normal",
                weights = 1/ms_timecourse$wts, 
                data = ms_timecourse)
toc()

elogit_jl_re_dataset_more_data <- augment(elogit_jl_re_dataset_more)

ggplot(elogit_jl_re_dataset_more_data, 
       aes(x = t_window, y = elogit, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(aes(y = .fitted), fun = mean, geom = "line") +
  geom_hline(yintercept = 0, col = "black", lty = 2) +
  # ylim(0, 1) +
  xlab("Time (ms)") +
  ylab("Proportion Target Looking") 
```
This model manages to get the (perhaps nonsensical, dataset-driven) difference between younger and older kids, but it loses badly in BIC compared with the more parsimonious model with a bunch of random intercepts. 

# Final model

In the end, more simple random intercepts win over the others. 

```{r}
ms_timecourse$tw <- ms_timecourse$t_window
ms_timecourse$tw2 <- ms_timecourse$t_window^2
ms_timecourse$tw3 <- ms_timecourse$t_window^3
ms_timecourse$tw4 <- ms_timecourse$t_window^4
ms_timecourse$age_centered2 <- ms_timecourse$age_centered^2

elogit_jl <- jglmm(elogit ~ 
                     0 + tw + tw2 + tw3 + tw4 + 
                         tw:age_centered + 
                     tw2:age_centered +
                     tw3:age_centered +
                     tw4:age_centered +
                     tw:age_centered2 + 
                     tw2:age_centered2 + 
                     tw3:age_centered2 +
                     tw4:age_centered2 +
                  (1 | administration_id) +
                  (1 | dataset_id) +
                  (1 | english_stimulus_label),
                family = "normal",
                weights = 1/ms_timecourse$wts, 
                data = ms_timecourse)

elogit_jl_data <- augment(elogit_jl)
elogit_jl_data$fitted_prop <- elogit_to_prop(elogit_jl_data$.fitted, 
                                             elogit_jl_data$N)
```
Last piece is to look at how this looks in predictions ... showing that we can factor out some of the nutso stuff between the datasets.

```{r}
elogit_jl_coefs <- tidy(elogit_jl)
elogit_jl_data <- augment(elogit_jl)
elogit_jl_formula <- as.formula("elogit ~ 0 + tw + tw2 + tw3 + tw4 + 
                         tw:age_centered + 
                     tw2:age_centered +
                     tw3:age_centered +
                     tw4:age_centered +
                     tw:age_centered2 + 
                     tw2:age_centered2 + 
                     tw3:age_centered2 +
                     tw4:age_centered2")
elogit_jl_cells <- ms_timecourse %>%
  ungroup() %>%
  select(age_centered, age_centered, age_centered2, tw, tw2, tw3, tw4) %>%
  distinct() %>%
  mutate(elogit = NA)

elogit_to_prop2 <- VWPre::make_pelogit_fnc(ObsPerBin = 4, Constant = .5)
elogit_jl_data$fitted_prop <- elogit_to_prop(elogit_jl_data$.fitted, 
                                             elogit_jl_data$N)

elogit_jl_fits <- jglmm_predict_fixed(elogit_jl_coefs, 
                                      elogit_jl_formula, elogit_jl_cells) %>%
  mutate(age = age_centered + 36, 
         fitted_prop = elogit_to_prop(.fitted, 4)) %>%
  filter(age %in% c(18,30,42,54))
```

Final plot: proportions:

```{r}
p1 <- ggplot(elogit_jl_data, 
       aes(x = t_window, y = prop_target, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(aes(y = fitted_prop), fun = mean, geom = "line") +
  geom_hline(yintercept = 0.5, col = "black", lty = 2) +
  xlab("Time (ms)") +
  ylab("Proportion Target Looking") + 
  scale_color_solarized(name = "Age Group") + 
  theme(legend.position = "bottom")

p2 <- ggplot(elogit_jl_fits, 
       aes(x = tw, y = fitted_prop, col = factor(age))) + 
  geom_line() + 
  geom_hline(yintercept = .5, col = "black", lty = 2) +
  scale_color_solarized(name = "Age") + 
  xlab("Time (ms)") +
  ylab("Proportion Target Looking") + 
  theme(legend.position = "bottom")

cowplot::plot_grid(p1, p2)
```

```{r}
p1 <- ggplot(elogit_jl_data, 
       aes(x = t_window, y = elogit, col = age_binned)) + 
  stat_summary(fun.data = mean_se, geom = "pointrange") +
  stat_summary(aes(y = .fitted), fun = mean, geom = "line") +
  geom_hline(yintercept = 0, col = "black", lty = 2) +
  xlab("Time (ms)") +
  ylab("Target Looking (elogit)") + 
  scale_color_solarized(name = "Age Group") + 
  theme(legend.position = "none") + 
  ggrepel::geom_label_repel(data = filter(elogit_jl_data, t_window == 750) %>%
                              group_by(age_binned) %>% 
                              summarise(t_window = mean(t_window),
                                        elogit = mean(elogit)), 
                            aes(label = paste(age_binned,"mo")), 
                            nudge_x = c(100,100,-100,-100))

p2 <- ggplot(elogit_jl_fits, 
       aes(x = tw, y = .fitted, col = factor(age))) + 
  geom_line() + 
  geom_hline(yintercept = 0, col = "black", lty = 2) +
  scale_color_solarized(name = "Age") + 
  xlab("Time (ms)") +
  ylab("Target Looking (elogit)") + 
  theme(legend.position = "none")

p <- cowplot::plot_grid(p1, p2, labels = c("A","B"))
p
ggsave(p, filename = here("figures/age_gca.pdf"),
       width = 7, height = 3)
```
